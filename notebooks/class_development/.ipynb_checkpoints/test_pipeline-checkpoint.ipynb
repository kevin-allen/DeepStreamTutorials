{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9029b471-b328-4c9c-acef-954fa555f593",
   "metadata": {},
   "source": [
    "# Ds_infer_pipeline\n",
    "\n",
    "Class to build different pipelines. We need a python class that will deal with the deepstream pipeline, number of cameras, output video file, and output object file.\n",
    "\n",
    "This will simplify the rest of the positrack3 code.\n",
    "\n",
    "Sources:\n",
    "One or several cameras (usb or CSI)\n",
    "One or several files\n",
    "\n",
    "Pre-inference image processing \n",
    "Crop, resize\n",
    "\n",
    "Infer\n",
    "Any unet model\n",
    "\n",
    "File Outputs\n",
    "1. Video file with one or more video sources\n",
    "2. File with object position for each frame with timestamps. Use CV2 blob dÃ©tection, time, X, y, prob per object per frame. \n",
    "\n",
    "Display output.\n",
    "Display with blob.\n",
    "\n",
    "Config files for :\n",
    "1. Inference engine\n",
    "2. Sources and preprocessing\n",
    "3. Blob processing\n",
    "4. Output file format.\n",
    "\n",
    "Yalm file for pipeline configuration.\n",
    "\n",
    "Expose the model output for a batch when it becomes available.\n",
    "\n",
    "Work for any unet network. \n",
    "\n",
    "We just need a way so that it makes the new output tensor, frame no, time stamp available when a batch is processed. \n",
    "\n",
    "The code outside of this class can deal with model specific computations (animal position, hd), ttl pulses and ROS posting. Similar to what the current positrack2 programs.\n",
    "\n",
    "With this design, we can change the tracking process without having to change much code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28915c70-7e55-4a04-8aea-f692fa761f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../apps/')\n",
    "import gi\n",
    "import math\n",
    "import ctypes\n",
    "\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import GLib, Gst\n",
    "from common.is_aarch_64 import is_aarch64\n",
    "from common.bus_call import bus_call\n",
    "import cv2\n",
    "import pyds\n",
    "import numpy as np\n",
    "import os.path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e176f751-e63b-4c6a-bfd9-33e82e4ff627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepstream_pipeline:\n",
    "    \"\"\"\n",
    "    Class use to build a Gstreamer/Deepstream pipeline.\n",
    "    The configuration of the pipeline is described in a few config files.\n",
    "    The inference_config_file is in a format prescribed by NVIDIA Deepstream.\n",
    "    The other config files are yalm file that were develop to work with this class\n",
    "    \n",
    "    This class should allow some flexibility when building the pipeline so that we can use the same class across different experiments.\n",
    "    \n",
    "    Here is an idea of what this class deal with.\n",
    "    \n",
    "    Source:\n",
    "    Can be one or several cameras\n",
    "    Can be one or several video files\n",
    "    \n",
    "    Preprocessing:\n",
    "    Crop, rotate and resize operations\n",
    "    \n",
    "    \n",
    "    Inference:\n",
    "    TensorRT model\n",
    "    \n",
    "    \n",
    "    Model output processing:\n",
    "    Steps apply to the model output to obtain the position of objects\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    Window with images\n",
    "    File with position of object in each video frames\n",
    "    File with the images processed\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pipeline_config_file):\n",
    "    \n",
    "        self.pipeline_config_file = pipeline_config_file\n",
    "       \n",
    "        \n",
    "        if not os.path.exists(self.pipeline_config_file):\n",
    "            raise IOError(f\"{self.pipeline_config_file} missing\")\n",
    "\n",
    "        self.load_pipeline_config()\n",
    "\n",
    "         # Standard GStreamer initialization\n",
    "        Gst.init(None)\n",
    "\n",
    "        # Create gstreamer elements\n",
    "        # Create Pipeline element that will form a connection of other elements\n",
    "        print(\"Creating Pipeline \\n \")\n",
    "        self.pipeline = Gst.Pipeline()\n",
    "\n",
    "        if not self.pipeline:\n",
    "            raise ValueError(\" Unable to create Pipeline \\n\")\n",
    "\n",
    "        \n",
    "        # add all the elements in the pipeline\n",
    "        self.add_source_elements()\n",
    "        self.add_streammux()\n",
    "        self.add_inference()  \n",
    "        self.add_output_elements()\n",
    "        \n",
    "        # link all elements of the pipeline\n",
    "        self.link_pipeline_elements()\n",
    "        \n",
    "        \n",
    "        loop = GLib.MainLoop()\n",
    "        bus = self.pipeline.get_bus()\n",
    "        bus.add_signal_watch()\n",
    "        bus.connect (\"message\", bus_call, loop)\n",
    "        \n",
    "        self.pipeline.set_state(Gst.State.PLAYING)\n",
    "        try:\n",
    "            loop.run()\n",
    "        except:\n",
    "            pass\n",
    "        # cleanup\n",
    "        print(\"Cleanup\")\n",
    "        self.pipeline.set_state(Gst.State.NULL)\n",
    "        \n",
    "   \n",
    "    def link_pipeline_elements(self):\n",
    "        \"\"\"\n",
    "        Link the elements of the pipeline\n",
    "        \n",
    "        After adding all elements to the pipeline, we need to link them\n",
    "        \n",
    "        \"\"\"\n",
    "        self.link_sources() # link the source elements, and the sources to the streammux\n",
    "    \n",
    "        self.streammux.link(self.segmentation)\n",
    "            \n",
    "        if self.output_number > 1:\n",
    "            # link the segmentation to the tee\n",
    "            pass\n",
    "        \n",
    "        self.link_outputs()\n",
    "   \n",
    "\n",
    "    def link_sources(self):\n",
    "        \"\"\"\n",
    "        Link the elements of the sources\n",
    "        \"\"\"\n",
    "        for i in range(self.source_number):\n",
    "            s = self.source_list[i]\n",
    "            if s[\"type\"]==\"usb\":\n",
    "                print(f\"Link source {i} usb camera\")\n",
    "                ## add elements to the pipeline\n",
    "                s[\"source\"].link(s[\"caps_v4l2src\"])\n",
    "                s[\"caps_v4l2src\"].link(s[\"vidconvsrc\"])\n",
    "                s[\"vidconvsrc\"].link(s[\"nvvidconvsrc\"])\n",
    "                s[\"nvvidconvsrc\"].link(s[\"caps_vidconvsrc\"])\n",
    "                \n",
    "                ## link the source to the streammux\n",
    "                s[\"caps_vidconvsrc_srcpad\"] = s[\"caps_vidconvsrc\"].get_static_pad(\"src\")\n",
    "                if not s[\"caps_vidconvsrc_srcpad\"]:\n",
    "                    raise ValueError(\"Unable to get source pad of caps_vidconvsrc \\n\")\n",
    "                \n",
    "                self.streammux_sinkpad = self.streammux.get_request_pad(\"sink_0\")\n",
    "                if not self.streammux_sinkpad:\n",
    "                    raise ValueError(\"Unable to get the sink pad of streammux \\n\")\n",
    "\n",
    "                \n",
    "                \n",
    "                s[\"caps_vidconvsrc_srcpad\"].link(self.streammux_sinkpad)\n",
    "    \n",
    "    def link_outputs(self):\n",
    "        \"\"\"\n",
    "        Link the elements of the outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.output_number > 1:\n",
    "            self.segmentation.link(self.output_tee)\n",
    "        \n",
    "        \n",
    "        for i in range(self.output_number):\n",
    "            out = self.output_list[i]\n",
    "            \n",
    "            if out[\"type\"] == \"video_file\":\n",
    "                print(f\"Linking video_file {i}\")\n",
    "                if self.output_number > 1:\n",
    "                    \n",
    "                    tee_file_pad = self.output_tee.get_request_pad('src_%u')\n",
    "                    if not tee_file_pad:\n",
    "                        raise ValueError(\"Unable to get request tee pads\\n\")\n",
    "                    sink_queue_pad = out[\"queue\"].get_static_pad('sink')\n",
    "                    if not sink_queue_pad:\n",
    "                        raise ValueError(\"Unable to get request sink_queue1 pad\\n\")\n",
    "                    tee_file_pad.link(sink_queue_pad)\n",
    "                \n",
    "                else:\n",
    "                    ## link to the segmentation\n",
    "                    print(\"Link to self.segmentation\")\n",
    "                    self.segmentation.link(out[\"queue\"])\n",
    "                \n",
    "                out[\"queue\"].link(out[\"nvvidconv\"])\n",
    "                out[\"nvvidconv\"].link(out[\"caps_vidconv\"])\n",
    "                out[\"caps_vidconv\"].link(out[\"nvvenc\"])\n",
    "                out[\"nvvenc\"].link(out[\"parse\"])\n",
    "                out[\"parse\"].link(out[\"mux\"])\n",
    "                out[\"mux\"].link(out[\"file_sink\"])\n",
    "            \n",
    "            if out[\"type\"] == \"on-screen-display\":\n",
    "                print(f\"Linking on-screen-display {i}\")\n",
    "                if self.output_number > 1:\n",
    "                    \n",
    "                    tee_osd_pad = self.output_tee.get_request_pad('src_%u')\n",
    "                    if not tee_osd_pad:\n",
    "                        raise ValueError(\"Unable to get request tee pads\\n\")\n",
    "                    sink_queue_pad = out[\"queue\"].get_static_pad('sink')\n",
    "                    if not sink_queue_pad:\n",
    "                        raise ValueError(\"Unable to get request sink_queue1 pad\\n\")\n",
    "                    tee_osd_pad.link(sink_queue_pad)\n",
    "                \n",
    "                else:\n",
    "                    ## link to the segmentation\n",
    "                    print(\"Link to self.segmentation\")\n",
    "                    self.segmentation.link(out[\"queue\"])\n",
    "                \n",
    "                out[\"queue\"].link(out[\"nvosd\"])\n",
    "                out[\"nvosd\"].link(out[\"sink_osd\"])\n",
    "                \n",
    "                \n",
    "    \n",
    "    def add_streammux(self):\n",
    "        \"\"\"\n",
    "        Add a streammux before the inference element\n",
    "        \"\"\"\n",
    "        self.streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "        if not self.streammux:\n",
    "            raise ValueError(\" Unable to create NvStreamMux \\n\")\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.streammux.set_property(\"nvbuf-memory-type\", int(pyds.NVBUF_MEM_CUDA_DEVICE))\n",
    "        self.streammux.set_property('width', self.streammux_properties[\"width\"])\n",
    "        self.streammux.set_property('height', self.streammux_properties[\"height\"])\n",
    "        self.streammux.set_property('batch-size', self.streammux_properties[\"batch-size\"])\n",
    "        self.streammux.set_property('batched-push-timeout', self.streammux_properties[\"batched-push-timeout\"])\n",
    "        self.streammux.set_property('live-source', self.streammux_properties[\"live-source\"]) # Essential to get more than 1 Hz\n",
    "    \n",
    "        \n",
    "        self.pipeline.add(self.streammux)\n",
    "     \n",
    "    def add_inference(self):\n",
    "        \"\"\"\n",
    "        Add the inference element to the pipeline\n",
    "        \"\"\"\n",
    "        # Create segmentation for primary inference\n",
    "        self.segmentation = Gst.ElementFactory.make(\"nvinfer\", \"primary-nvinference-engine\")\n",
    "        \n",
    "        # Get configuration from the config file\n",
    "        if not self.segmentation:\n",
    "            raise ValueError(\"Unable to create primary inferene\\n\")\n",
    "        self.segmentation.set_property('config-file-path', self.inference_config_file)\n",
    "        pgie_batch_size = self.segmentation.get_property(\"batch-size\")\n",
    "        \n",
    "        if pgie_batch_size != self.source_number:\n",
    "            print(\"WARNING: Overriding infer-config batch-size\", pgie_batch_size,\n",
    "                  \" with number of sources \", num_sources,\n",
    "                  \" \\n\")\n",
    "            self.segmentation.set_property(\"batch-size\", num_sources)\n",
    "        \n",
    "        self.pipeline.add(self.segmentation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_source_elements(self):\n",
    "        \"\"\"\n",
    "        Add source elements to the pipeline\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for i in range(self.source_number):\n",
    "            s = self.source_list[i]\n",
    "            if s[\"type\"]==\"usb\":\n",
    "                \n",
    "                ## create elements\n",
    "                s[\"source\"] = Gst.ElementFactory.make(\"v4l2src\", f\"usb-cam-source{i}\")\n",
    "                if not s[\"source\"]:\n",
    "                    raise ValueError(\" Unable to create Source \\n\")\n",
    "                    \n",
    "                s[\"caps_v4l2src\"] = Gst.ElementFactory.make(\"capsfilter\", f\"v4l2src_caps{i}\")\n",
    "                if not s[\"caps_v4l2src\"]:\n",
    "                    raise ValueError(\" Unable to create caps_v4l2src\\n\")\n",
    "                \n",
    "                s[\"vidconvsrc\"] = Gst.ElementFactory.make(\"videoconvert\", f\"convertor_src{i}\")\n",
    "                if not s[\"vidconvsrc\"]:\n",
    "                    raise ValueError(\" Unable to create vidconvsrc\\n\")\n",
    "                \n",
    "                \n",
    "                s[\"nvvidconvsrc\"] = Gst.ElementFactory.make(\"nvvideoconvert\", f\"nvvidconv_src{i}\")\n",
    "                if not s[\"nvvidconvsrc\"]:\n",
    "                    raise ValueError(\" Unable to create nvvidconvsrc\\n\") \n",
    "                \n",
    "                s[\"caps_vidconvsrc\"] = Gst.ElementFactory.make(\"capsfilter\", f\"nvmm_caps{i}\")\n",
    "                if not s[\"caps_vidconvsrc\"]:\n",
    "                    raise ValueError(\" Unable to create caps_vidconvsrc\\n\")\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "                ## set properties\n",
    "                fr = s[\"framerate\"]\n",
    "                s[\"caps_v4l2src\"].set_property('caps', Gst.Caps.from_string(f\"video/x-raw, framerate={fr}/1\"))\n",
    "                s[\"caps_vidconvsrc\"].set_property('caps', Gst.Caps.from_string(\"video/x-raw(memory:NVMM)\"))\n",
    "                s[\"source\"].set_property('device', s[\"path\"])\n",
    "                \n",
    "                ## add elements to the pipeline\n",
    "                self.pipeline.add(s[\"source\"])\n",
    "                self.pipeline.add(s[\"caps_v4l2src\"])\n",
    "                self.pipeline.add(s[\"vidconvsrc\"])\n",
    "                self.pipeline.add(s[\"nvvidconvsrc\"])\n",
    "                self.pipeline.add(s[\"caps_vidconvsrc\"])\n",
    "    \n",
    "    \n",
    "\n",
    "    def add_output_elements(self):\n",
    "        \"\"\"\n",
    "        Add output element to the pipeline\n",
    "        \"\"\"\n",
    "        if self.output_number > 1:\n",
    "            # we will need a tee\n",
    "            self.output_tee =  Gst.ElementFactory.make(\"tee\", \"tee1\")\n",
    "            if not self.output_tee:\n",
    "                raise ValueError(\" Unable to create tee\\n\")\n",
    "            self.pipeline.add(self.output_tee)\n",
    "            \n",
    "        \n",
    "        for i in range(self.output_number):\n",
    "            out = self.output_list[i]\n",
    "            if out[\"type\"] == \"video_file\":\n",
    "                ## Create part of the pipeline to save a video file\n",
    "                \n",
    "                out[\"queue\"] =  Gst.ElementFactory.make(\"queue\", f\"queue_output{i}\")\n",
    "                if not out[\"queue\"]:\n",
    "                    raise ValueError(\"Unable to create queue\\n\")\n",
    "\n",
    "                out[\"nvvidconv\"] = Gst.ElementFactory.make(\"nvvideoconvert\", f\"convertor_output{i}\")\n",
    "                if not out[\"nvvidconv\"]:\n",
    "                    raise ValueError(\"Unable to create nvvideoconvert\\n\")\n",
    "\n",
    "                out[\"caps_vidconv\"] = Gst.ElementFactory.make(\"capsfilter\", f\"nvmm_caps_output{i}\")\n",
    "                if not out[\"caps_vidconv\"]:\n",
    "                    raise ValueError(\"Unable to create capsfilter\\n\")\n",
    "\n",
    "                out[\"nvvenc\"] = Gst.ElementFactory.make(\"nvv4l2h264enc\", f\"enc_output{i}\")\n",
    "                if not out[\"nvvenc\"]:\n",
    "                    raise ValueError(\"Unable to create nvvenc\\n\")\n",
    "\n",
    "                out[\"parse\"] = Gst.ElementFactory.make(\"h264parse\", f\"parse_output{i}\")\n",
    "                if not out[\"parse\"]:\n",
    "                    raise ValueError(\"Unable to create parse\\n\")\n",
    "\n",
    "                out[\"mux\"] =  Gst.ElementFactory.make(\"matroskamux\", f\"mux_output{i}\")\n",
    "                if not out[\"mux\"]:\n",
    "                    raise ValueError(\"Unable to create mux\\n\")\n",
    "\n",
    "                out[\"file_sink\"] =  Gst.ElementFactory.make(\"filesink\", f\"fs_output{i}\")\n",
    "                if not out[\"file_sink\"]:\n",
    "                    raise ValueError(\" Unable to create file_sink\\n\")\n",
    "\n",
    "                width=out[\"width\"]\n",
    "                height=out[\"height\"]\n",
    "                out[\"caps_vidconv\"].set_property('caps', \n",
    "                                                 Gst.Caps.from_string(f'video/x-raw(memory:NVMM),width={width}, height={height}'))\n",
    "                \n",
    "                \n",
    "                out[\"nvvenc\"].set_property('bitrate', out[\"bitrate\"])\n",
    "                out[\"file_sink\"].set_property(\"location\", out[\"default-file-name\"])\n",
    "\n",
    "                \n",
    "                self.pipeline.add(out[\"queue\"])\n",
    "                self.pipeline.add(out[\"nvvidconv\"])\n",
    "                self.pipeline.add(out[\"caps_vidconv\"])\n",
    "                self.pipeline.add(out[\"nvvenc\"])\n",
    "                self.pipeline.add(out[\"parse\"])\n",
    "                self.pipeline.add(out[\"mux\"])\n",
    "                self.pipeline.add(out[\"file_sink\"])\n",
    "\n",
    "            if out[\"type\"] == \"on-screen-display\":\n",
    "                \n",
    "                out[\"queue\"] =  Gst.ElementFactory.make(\"queue\", f\"queue_output{i}\")\n",
    "                if not out[\"queue\"]:\n",
    "                    raise ValueError(\"Unable to create queue\\n\")\n",
    "                \n",
    "                out[\"nvosd\"] = Gst.ElementFactory.make(\"nvdsosd\", f\"onscreendisplay_output{i}\")\n",
    "                if not out[\"nvosd\"]:\n",
    "                    raise ValueError(\"Unable to create nvosd\\n\")\n",
    "                    \n",
    "                out[\"sink_osd\"] = Gst.ElementFactory.make(\"nveglglessink\", f\"nvvideo-renderer-osd_output{i}\")\n",
    "                if not out[\"sink_osd\"]:\n",
    "                    raise ValueError(\"Unable to create egl sink_seg \\n\")\n",
    "                \n",
    "                \n",
    "                out[\"sink_osd\"].set_property(\"qos\", 0)\n",
    "                \n",
    "                self.pipeline.add(out[\"queue\"])\n",
    "                self.pipeline.add(out[\"nvosd\"])\n",
    "                self.pipeline.add(out[\"sink_osd\"])\n",
    "                \n",
    "        \n",
    "                        \n",
    "    def load_pipeline_config(self):\n",
    "        \"\"\"\n",
    "        Load the configuration from a config.yaml file\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.pipeline_config_file):\n",
    "            print(\"Loading\",self.pipeline_config_file)\n",
    "            with open(self.pipeline_config_file) as file:\n",
    "                self.configDict = yaml.full_load(file)\n",
    "    \n",
    "    \n",
    "        # process sources\n",
    "        print(self.configDict)\n",
    "        self.source_number=self.configDict[\"source_number\"]\n",
    "        self.source_list = []\n",
    "        for i in range(self.source_number):\n",
    "            self.source_list.append({\"path\":self.configDict[f\"source{i}_path\"],\n",
    "                                     \"type\":self.configDict[f\"source{i}_type\"],\n",
    "                                     \"framerate\":self.configDict[f\"source{i}_framerate\"]})\n",
    "        # process streammux\n",
    "        self.streammux_properties = {\"width\":self.configDict[\"streammux_width\"],\n",
    "                                     \"height\":self.configDict[\"streammux_height\"],\n",
    "                                     \"batch-size\":self.configDict[\"streammux_batch-size\"],\n",
    "                                     \"batched-push-timeout\":self.configDict[\"streammux_batched-push-timeout\"],\n",
    "                                     \"live-source\":self.configDict[\"streammux_live-source\"]}\n",
    "        # inference\n",
    "        self.inference_config_file = self.configDict[\"inference_config_file\"]\n",
    "        \n",
    "        # output\n",
    "        self.output_number = self.configDict[\"output_number\"]\n",
    "        self.output_list = []\n",
    "        for i in range(self.output_number):\n",
    "            self.output_list.append({\"type\":self.configDict[f\"output{i}_type\"]})\n",
    "            # type specific fields\n",
    "            if self.output_list[-1][\"type\"] == \"video_file\":\n",
    "                self.output_list[-1][\"width\"] = self.configDict[f\"output{i}_width\"],\n",
    "                self.output_list[-1][\"height\"] = self.configDict[f\"output{i}_height\"]\n",
    "                self.output_list[-1][\"bitrate\"] = self.configDict[f\"output{i}_bitrate\"]\n",
    "                self.output_list[-1][\"default-file-name\"] = self.configDict[f\"output{i}_default-file-name\"]\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9302a72-8021-4e0b-8bf5-9e342244f83b",
   "metadata": {},
   "source": [
    "Start with a minimal example\n",
    "\n",
    "* 1 usb camera\n",
    "* facetrack\n",
    "* save to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a194117f-79cc-4656-af27-96014809728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline_config.yalm\n",
      "{'source_number': 1, 'source0_path': '/dev/video0', 'source0_type': 'usb', 'source0_framerate': 30, 'streammux_width': 1920, 'streammux_height': 1080, 'streammux_batch-size': 1, 'streammux_batched-push-timeout': 4000, 'streammux_live-source': 1, 'inference_config_file': 'inference_config.txt', 'output_number': 2, 'output0_type': 'video_file', 'output0_width': 640, 'output0_height': 480, 'output0_bitrate': 4000000, 'output0_default-file-name': 'default_video.mp4', 'output1_type': 'on-screen-display'}\n",
      "Creating Pipeline \n",
      " \n",
      "Link source 0 usb camera\n",
      "Linking video_file 0\n",
      "Linking on-screen-display 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53:31:53.202262568 \u001b[332m 3499\u001b[00m      0x2c9d360 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:646:gst_nvinfer_logger:<primary-nvinference-engine>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::deserializeEngineAndBackend() <nvdsinfer_context_impl.cpp:1909> [UID = 1]: deserialized trt engine from :/home/kevin/repo/DeepStreamTutorials/models/serialized_UNet_engine.trt\n",
      "ERROR: [TRT]: 3: Cannot find binding of given name: softmax_1\n",
      "53:31:53.227686327 \u001b[332m 3499\u001b[00m      0x2c9d360 \u001b[33;01mWARN   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:643:gst_nvinfer_logger:<primary-nvinference-engine>\u001b[00m NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::checkBackendParams() <nvdsinfer_context_impl.cpp:1876> [UID = 1]: Could not find output layer 'softmax_1' in engine\n",
      "53:31:53.227715606 \u001b[332m 3499\u001b[00m      0x2c9d360 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:646:gst_nvinfer_logger:<primary-nvinference-engine>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::generateBackendContext() <nvdsinfer_context_impl.cpp:2012> [UID = 1]: Use deserialized engine model: /home/kevin/repo/DeepStreamTutorials/models/serialized_UNet_engine.trt\n",
      "53:31:53.239029301 \u001b[332m 3499\u001b[00m      0x2c9d360 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer_impl.cpp:328:notifyLoadModelStatus:<primary-nvinference-engine>\u001b[00m [UID 1]: Load new model:inference_config.txt sucessfully\n",
      "Error: gst-resource-error-quark: Output window was closed (3): ext/eglgles/gsteglglessink.c(893): gst_eglglessink_event_thread (): /GstPipeline:pipeline9/GstEglGlesSink:nvvideo-renderer-osd_output1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup\n"
     ]
    }
   ],
   "source": [
    "pipe = Deepstream_pipeline(pipeline_config_file=\"pipeline_config.yalm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2dc4f-3df7-451c-a4a2-cdeed538efbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
